{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Run Video Inference.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjcommand/annotation-python-tools/blob/main/notebooks/Run%20Video%20Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video Detection and Tracking Inference\n",
        "#### Peyton Lee and Neha Nagvekar, 5/31/22\n",
        "\n",
        "Runs inference on a provided video using a trained YOLOv5 model and the Norfair tracking algorithm."
      ],
      "metadata": {
        "id": "4ZEj53rx6anC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "You'll need to download and install the Deepsea-Detector project, which you can find on [GitHub](https://github.com/ShrimpCryptid/deepsea-detector)!"
      ],
      "metadata": {
        "id": "lNsqBUB5v25H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2Y05ginU6N3V",
        "outputId": "5937cd35-4027-478b-9008-406c5a56443d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deepsea-detector'...\n",
            "remote: Enumerating objects: 432, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 432 (delta 24), reused 19 (delta 6), pack-reused 378 (from 1)\u001b[K\n",
            "Receiving objects: 100% (432/432), 38.74 MiB | 8.58 MiB/s, done.\n",
            "Resolving deltas: 100% (226/226), done.\n",
            "Updating files: 100% (74/74), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ShrimpCryptid/deepsea-detector.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the Deepsea-Detector project if already downloaded!\n",
        "!cd deepsea-detector; git pull"
      ],
      "metadata": {
        "id": "SqN3j5eJPRiw",
        "outputId": "b42dc57b-df38-4049-d1ff-d33807f9d7f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This project uses Git large file system (LFS), so we need to install it and\n",
        "# fetch our large project files with it.\n",
        "!cd deepsea-detector; git lfs install; git lfs fetch; git lfs pull"
      ],
      "metadata": {
        "id": "dKson8CK19Cr",
        "outputId": "47561d36-d9a0-4e54-f695-75fcee5cbfb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "fetch: Fetching reference refs/heads/main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Deepsea-Detector's dependencies.\n",
        "!pip install -r deepsea-detector/requirements.txt -q"
      ],
      "metadata": {
        "id": "5ee0SWj56uej",
        "outputId": "ae5e7a2e-2677-42f8-d9e8-797d51a7a4fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.4/953.4 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.2/881.2 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc 5.17.0 requires rich>=13.7.1, but you have rich 12.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Setup\n",
        "You'll need to do the following steps before processing a video.\n",
        "1. Import your model weights, which should end in `.pt`. You can use the default model included with Deepsea-Detector, or upload your own.\n",
        "2. Import the video file(s) you want to analyze."
      ],
      "metadata": {
        "id": "fFDxdCoW61vZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, you can download your model weights and video files from an external source using the `curl` command, or upload files to the environment.\n",
        "\n",
        "If you're using Google Colab, you can upload videos using the file browser on the left pane. *(Be warned that downloading/uploading files from Colab can be rather slow.)*"
      ],
      "metadata": {
        "id": "rHI4JtyzxKzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download an example video via URL and save it as `video.mp4`.\n",
        "video_url = \"https://data.nodc.noaa.gov/oer/video/EX1708/Video/EX1708_DIVE01_20170907/Compressed/EX1708_VID_20170907T204000Z_ROVHD_Low.mp4\"\n",
        "!curl -L {video_url} --output video.mp4"
      ],
      "metadata": {
        "id": "7pMtFcbn745D",
        "outputId": "46ec186f-e63d-4065-e638-ce7acc2a8c2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0   344    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 56.9M  100 56.9M    0     0  17.5M      0  0:00:03  0:00:03 --:--:-- 26.4M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also mount a Google Drive folder to access files, which is much faster than directly uploading/downloading them from Google Colab. You'll get prompted to authorize Google Colab to make changes to your Drive."
      ],
      "metadata": {
        "id": "sKlA8bUzxfLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and mount your Google Drive folder as a directory that Colab can access.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7yUqVwaHO0Pi",
        "outputId": "5f57664f-d663-4ab3-f17d-49f1fcd98f8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3d6d616c60e0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import and mount your Google Drive folder as a directory that Colab can access.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# check if cuda is available.\n",
        "device=\"cpu\"\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Warning: CUDA is not available. The script will be slower than with CUDA. To enable cuda, go to Runtime -> Change runtime type -> Hardware accelerator -> GPU\")\n",
        "else:\n",
        "    print(\"CUDA is available.\")\n",
        "    device=\"cuda\""
      ],
      "metadata": {
        "id": "3nBfOKDnrBLN",
        "outputId": "291e00bf-a19b-4d75-ebff-ac400aa033df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing a Video\n",
        "The following script runs the inference for a single video."
      ],
      "metadata": {
        "id": "CtCR48lkzVXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add apostrophes (') on either side of the path if there are spaces in any of your path names.\n",
        "MODEL_PATH = \"/content/deepsea-detector/models/deepsea-detector.pt\"\n",
        "VIDEO_INPUT_PATH = \"/content/deepsea-detector/data/test_input_video/coral_trimmed.mp4\"\n",
        "VIDEO_OUTPUT_PATH = \"single_out.mp4\"\n",
        "CSV_OUTPUT_PATH = \"single_out.csv\"\n",
        "\n",
        "!python3 deepsea-detector/src/detection.py {VIDEO_INPUT_PATH} \\\n",
        "--detector_path {MODEL_PATH} \\\n",
        "--img_size 640 \\\n",
        "--conf_thres 0.10 \\\n",
        "--period 3 \\\n",
        "--device cpu \\\n",
        "--output_video {VIDEO_OUTPUT_PATH} \\\n",
        "--output_csv {CSV_OUTPUT_PATH}"
      ],
      "metadata": {
        "id": "EM3XWAcANkvb",
        "outputId": "b7b3440f-5f73-4324-b9b6-d32443efb902",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:root:You are using a scalar distance function. If you want to speed up the tracking process please consider using a vectorized distance function such as ['iou', 'iou_opt', 'braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'jensenshannon', 'kulczynski1', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'].\n",
            "\u001b[?25lc ... p4 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0.00fps\u001b[0m\r\u001b[2Kc ... p4 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m137.62fps\u001b[0m/usr/local/lib/python3.10/dist-packages/yolov5/models/common.py:709: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "WARNING:root:draw_tracked_boxes is deprecated, use draw_box instead\n",
            "\u001b[2Kc ... p4 \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  3%\u001b[0m \u001b[36m0:00:57\u001b[0m \u001b[33m2.73fps\u001b[0m/usr/local/lib/python3.10/dist-packages/yolov5/models/common.py:709: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "\u001b[2Kc ... p4 \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  6%\u001b[0m \u001b[36m0:00:55\u001b[0m \u001b[33m2.39fps\u001b[0m/usr/local/lib/python3.10/dist-packages/yolov5/models/common.py:709: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "\u001b[2Kc ... p4 \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  6%\u001b[0m \u001b[36m0:00:55\u001b[0m \u001b[33m2.39fps\u001b[0m\n",
            "\u001b[?25hTraceback (most recent call last):\n",
            "  File \"/content/deepsea-detector/src/detection.py\", line 255, in <module>\n",
            "    norfair.draw_tracked_boxes(frame, tracked_objects, border_colors=[(0, 255, 255)], border_width=1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/norfair/drawing/draw_boxes.py\", line 199, in draw_tracked_boxes\n",
            "    return draw_boxes(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/norfair/drawing/draw_boxes.py\", line 147, in draw_boxes\n",
            "    obj_color = parse_color(color)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/norfair/drawing/color.py\", line 259, in parse_color\n",
            "    return tuple([int(v) for v in color_like])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/norfair/drawing/color.py\", line 259, in <listcomp>\n",
            "    return tuple([int(v) for v in color_like])\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'tuple'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing Multiple Videos:\n",
        "Deepsea-Detector can also process multiple video files at once. To do this, you'll need to define an **output folder** and a **prefix** for the produced MP4 video files, instead of a single video output.\n",
        "\n",
        "The CSV data will be merged into a single file."
      ],
      "metadata": {
        "id": "Zl0oisy5wx3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"deepsea-detector/models/deepsea-detector.pt\"\n",
        "# Include a space between each video you want to process.\n",
        "VIDEO_INPUT_PATHS = \"deepsea-detector/data/test_input_video/coral_trimmed.mp4 deepsea-detector/data/test_input_video/shrimp_trimmed.mp4\"\n",
        "VIDEO_OUTPUT_FOLDER = \"example_output\"\n",
        "VIDEO_OUTPUT_PREFIX = \"multi_out_\"\n",
        "CSV_OUTPUT_PATH = \"multi_out.csv\"\n",
        "\n",
        "# Make an directory called example_output\n",
        "!mkdir example_output\n",
        "# Run the script for multiple input videos\n",
        "!python deepsea-detector/src/detection.py {VIDEO_INPUT_PATHS} \\\n",
        "--detector_path {MODEL_PATH} \\\n",
        "--img_size 640 \\\n",
        "--conf_thres 0.10 \\\n",
        "--period 3 \\\n",
        "--device cuda \\\n",
        "--output_folder {VIDEO_OUTPUT_FOLDER} \\\n",
        "--output_video_prefix {VIDEO_OUTPUT_PREFIX} \\\n",
        "--output_csv {CSV_OUTPUT_PATH}"
      ],
      "metadata": {
        "id": "GKAP8r-kyPih",
        "outputId": "84f774d5-1b03-4c8b-ee15-b09b67a7b687",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/deepsea-detector/src/detection.py\", line 173, in <module>\n",
            "    model = YOLO(args.detector_path, device=args.device)\n",
            "  File \"/content/deepsea-detector/src/detection.py\", line 50, in __init__\n",
            "    self.model = yolov5.load(model_path, device=device)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/yolov5/helpers.py\", line 35, in load_model\n",
            "    device = select_device(device)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/yolov5/utils/torch_utils.py\", line 118, in select_device\n",
            "    assert torch.cuda.is_available() and torch.cuda.device_count() >= len(device.replace(',', '')), \\\n",
            "AssertionError: Invalid CUDA '--device cuda' requested, use '--device cpu' or pass valid CUDA device(s)\n"
          ]
        }
      ]
    }
  ]
}